{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300b7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcae04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca5a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_date = \"2025-09-27\"\n",
    "symbol = \"ADAUSDT\"\n",
    "\n",
    "PROJECT_PREFIX = os.getenv(\"PROJECT_PREFIX\")\n",
    "PROJECT_PREFIX_UNDERSCORE = os.getenv(\"PROJECT_PREFIX_UNDERSCORE\")\n",
    "DATA_LAKE_BUCKET = os.getenv(\"DATA_LAKE_BUCKET\")\n",
    "ICEBERG_LOCK_TABLE = os.getenv(\"ICEBERG_LOCK_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c53840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        logger.info(f\"{file_name} exists\")\n",
    "        return\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        logger.info(\n",
    "            f\"Downloaded {file_name} {(os.path.getsize(file_name) / (1024 * 1024)):.2f}MB completed\"\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        logger.info(f\"{file_name} removed\")\n",
    "\n",
    "\n",
    "def extract_file(extract_dir, zip_path):\n",
    "    if not os.path.exists(zip_path):\n",
    "        logger.info(f\"{zip_path} not found\")\n",
    "        return\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "        csv_file = os.path.join(extract_dir, os.listdir(extract_dir)[0])\n",
    "        logger.info(f\"Extracted CSV: {csv_file}\")\n",
    "        return csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aead3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloading https://data.binance.vision/data/spot/daily/aggTrades/ADAUSDT/ADAUSDT-aggTrades-2025-09-27.zip -> ./ADAUSDT-aggTrades-2025-09-27.zip\n",
      "INFO:__main__:./ADAUSDT-aggTrades-2025-09-27.zip exists\n",
      "INFO:__main__:Extracted CSV: ./unzipped_data/ADAUSDT-aggTrades-2025-09-27.csv\n",
      "INFO:__main__:Download + Extract processed in 0.019 seconds\n"
     ]
    }
   ],
   "source": [
    "script_dir = \"./\"\n",
    "extract_dir = os.path.join(script_dir, \"unzipped_data\")\n",
    "url = f\"https://data.binance.vision/data/spot/daily/aggTrades/{symbol}/{symbol}-aggTrades-{landing_date}.zip\"\n",
    "file_name = os.path.join(script_dir, url.split(\"/\")[-1])\n",
    "logger.info(f\"Downloading {url} -> {file_name}\")\n",
    "\n",
    "start_t = time.time()\n",
    "download_file(url, file_name)\n",
    "csv_file = extract_file(extract_dir, file_name)\n",
    "end_t = time.time()\n",
    "logger.info(f\"Download + Extract processed in {(end_t - start_t):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36619a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/anhtu/.pyenv/versions/3.11.11/envs/crypto-cloud/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/anhtu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/anhtu/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a56b5b86-7303-4d37-b8ed-cd732f18154e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 86ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a56b5b86-7303-4d37-b8ed-cd732f18154e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/11/19 23:23:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Landing Job\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "    .config(\"spark.sql.columnVector.offheap.enabled\", \"false\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"false\")\n",
    "    .config(\"spark.sql.catalog.glue_catalog.read.parquet.vectorization.enabled\", \"false\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:MaxDirectMemorySize=1g\")\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2b5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"agg_trade_id\", types.LongType(), True),\n",
    "        types.StructField(\"price\", types.DoubleType(), True),\n",
    "        types.StructField(\"quantity\", types.DoubleType(), True),\n",
    "        types.StructField(\"first_trade_id\", types.LongType(), True),\n",
    "        types.StructField(\"last_trade_id\", types.LongType(), True),\n",
    "        types.StructField(\"timestamp\", types.LongType(), True),\n",
    "        types.StructField(\"is_buyer_maker\", types.BooleanType(), True),\n",
    "        types.StructField(\"is_best_match\", types.BooleanType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\").schema(schema).csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb121b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ingest_date\", F.current_date()).withColumn(\n",
    "    \"ingest_timestamp\", F.current_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18ab50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 23:23:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "INFO:__main__:Parquet written to: s3a://crypto-cloud-dev-650251698703-data-lake-bucket/landing_zone/spot/daily/aggTrades/ADAUSDT/2025-09-27\n"
     ]
    }
   ],
   "source": [
    "output_path = (\n",
    "    f\"s3a://{DATA_LAKE_BUCKET}/landing_zone/spot/daily/aggTrades/{symbol}/{landing_date}\"\n",
    ")\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "logger.info(f\"Parquet written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f9735e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:./unzipped_data/ADAUSDT-aggTrades-2025-09-27.csv removed\n",
      "INFO:__main__:./ADAUSDT-aggTrades-2025-09-27.zip removed\n"
     ]
    }
   ],
   "source": [
    "remove_file(csv_file)\n",
    "remove_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2667b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+--------------+-------------+----------------+--------------+-------------+-----------+--------------------------+\n",
      "|agg_trade_id|price |quantity|first_trade_id|last_trade_id|timestamp       |is_buyer_maker|is_best_match|ingest_date|ingest_timestamp          |\n",
      "+------------+------+--------+--------------+-------------+----------------+--------------+-------------+-----------+--------------------------+\n",
      "|411114767   |0.7918|111.5   |711706932     |711706932    |1758931200686229|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114768   |0.7918|252.5   |711706933     |711706933    |1758931200711075|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114769   |0.7918|240.6   |711706934     |711706937    |1758931202600822|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114770   |0.7917|43.1    |711706938     |711706940    |1758931204869784|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114771   |0.7917|22.6    |711706941     |711706941    |1758931204879746|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114772   |0.7917|669.9   |711706942     |711706945    |1758931205396640|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114773   |0.7916|81.2    |711706946     |711706949    |1758931205433486|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114774   |0.7917|70.1    |711706950     |711706951    |1758931209806960|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114775   |0.7917|624.0   |711706952     |711706954    |1758931211082402|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114776   |0.7917|31.6    |711706955     |711706955    |1758931211824816|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114777   |0.7916|82.6    |711706956     |711706962    |1758931214552496|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114778   |0.7916|499.5   |711706963     |711706964    |1758931215346625|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114779   |0.7916|50.0    |711706965     |711706966    |1758931215726986|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114780   |0.7916|4421.4  |711706967     |711706970    |1758931215971498|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114781   |0.7916|8.8     |711706971     |711706971    |1758931217816837|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114782   |0.7916|182.7   |711706972     |711706973    |1758931218847336|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114783   |0.7916|6.9     |711706974     |711706974    |1758931219320049|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114784   |0.7916|7.1     |711706975     |711706975    |1758931224606182|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114785   |0.7915|47.9    |711706976     |711706978    |1758931227296533|true          |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "|411114786   |0.7916|31.0    |711706979     |711706979    |1758931229299427|false         |true         |2025-11-19 |2025-11-19 16:23:20.973466|\n",
      "+------------+------+--------+--------------+-------------+----------------+--------------+-------------+-----------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_path).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-cloud-spark-jobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
