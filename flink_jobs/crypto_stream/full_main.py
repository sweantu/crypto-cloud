import json
import logging
import os

import pyflink
from pyflink.common import Types
from pyflink.datastream import (
    StreamExecutionEnvironment,
)
from pyflink.table import EnvironmentSettings, StreamTableEnvironment

from .indicators_function import IndicatorsFunction

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)  # match Kafka partitions
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Location of the configuration file when running on Managed Flink.
# NOTE: this is not the file included in the project, but a file generated by Managed Flink, based on the
# application configuration.
APPLICATION_PROPERTIES_FILE_PATH = "/etc/flink/application_properties.json"

# Set the environment variable IS_LOCAL=true in your local development environment,
# or in the run profile of your IDE: the application relies on this variable to run in local mode (as a standalone
# Python application, as opposed to running in a Flink cluster).
# Differently from Java Flink, PyFlink cannot automatically detect when running in local mode
is_local = True if os.environ.get("IS_LOCAL") else False
logger.info(f"Is local mode: {is_local} from logging")
print(f"Is local mode: {is_local} from print")

##############################################
# 2. Set special configuration for local mode
##############################################

if is_local:
    # Load the configuration from the json file included in the project
    APPLICATION_PROPERTIES_FILE_PATH = "application_properties.json"

    # Point to the fat-jar generated by Maven, containing all jar dependencies (e.g. connectors)
    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))
    t_env.get_config().get_configuration().set_string(
        "pipeline.jars",
        # For local development (only): use the fat-jar containing all dependencies, generated by `mvn package`
        # located in the target/ subdirectory
        "file:///" + CURRENT_DIR + "/target/pyflink-dependencies.jar",
    )

    # Show the PyFlink home directory and the directory where logs will be written, when running locally
    print("PyFlink home: " + os.path.dirname(os.path.abspath(pyflink.__file__)))
    print(
        "Logging directory: "
        + os.path.dirname(os.path.abspath(pyflink.__file__))
        + "/log"
    )


# Utility method, extracting properties from the runtime configuration file
def get_application_properties():
    if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH):
        with open(APPLICATION_PROPERTIES_FILE_PATH, "r") as file:
            contents = file.read()
            properties = json.loads(contents)
            return properties
    else:
        print('A file at "{}" was not found'.format(APPLICATION_PROPERTIES_FILE_PATH))


# Utility method, extracting a property from a property group
def property_map(props, property_group_id):
    for prop in props:
        if prop["PropertyGroupId"] == property_group_id:
            return prop["PropertyMap"]


AT_TIMESTAMP = "AT_TIMESTAMP"


def main():
    #####################################
    # 3. Retrieve runtime configuration
    #####################################

    props = get_application_properties()

    # Input stream configuration
    input_stream_properties = property_map(props, "InputStream0")
    input_stream_arn = input_stream_properties["stream.arn"]
    input_stream_region = input_stream_properties["aws.region"]
    input_stream_init_position = (
        input_stream_properties["flink.source.init.position"]
        if "flink.source.init.position" in input_stream_properties
        else None
    )
    input_stream_init_timestamp = (
        input_stream_properties["flink.source.init.timestamp"]
        if "flink.source.init.timestamp" in input_stream_properties
        else None
    )
    if (
        input_stream_init_position == AT_TIMESTAMP
        and input_stream_init_timestamp == None
    ):
        raise ValueError(
            f"A timestamp must be supplied for flink.source.init.position = {AT_TIMESTAMP}"
        )

    # Output stream configuration
    output_stream_properties = property_map(props, "OutputStream0")
    output_stream_arn = output_stream_properties["stream.arn"]
    output_stream_region = output_stream_properties["aws.region"]
    output_clickhouse_ip = output_stream_properties["clickhouse.ip"]
    logger.info(f"Output ClickHouse IP: {output_clickhouse_ip} from logging")
    print(f"Output ClickHouse IP: {output_clickhouse_ip} from print")

    #################################################
    # 4. Define source table using kinesis connector
    #################################################

    # Some trick is required to generate the string defining the initial position, depending on the configuration
    # See Flink documentation for further details about configuring a Kinesis source table
    # https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/kinesis/
    source_init_pos = (
        "\n'source.init.position' = '{0}',".format(input_stream_init_position)
        if input_stream_init_position is not None
        else ""
    )
    source_init_timestamp = (
        "\n'source.init.timestamp' = '{0}',".format(input_stream_init_timestamp)
        if input_stream_init_timestamp is not None
        else ""
    )

    # --- 2Ô∏è‚É£ Kafka source ---
    t_env.execute_sql("DROP TABLE IF EXISTS aggtrades_source")
    t_env.execute_sql(f"""
    CREATE TABLE aggtrades_source (
        agg_trade_id BIGINT,
        price DOUBLE,
        quantity DOUBLE,
        first_trade_id BIGINT,
        last_trade_id BIGINT,
        ts_int BIGINT,
        is_buyer_maker BOOLEAN,
        is_best_match BOOLEAN,
        symbol STRING,
        ts AS TO_TIMESTAMP_LTZ(ts_int / 1000, 3),
        WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
    ) WITH (
        'connector' = 'kinesis',
        'stream.arn' = '{input_stream_arn}',
        'aws.region' = '{input_stream_region}',
        {source_init_pos}{source_init_timestamp}
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
    """)

    # --- 3Ô∏è‚É£ Klines view ---
    t_env.execute_sql("DROP TEMPORARY VIEW IF EXISTS klines_view")
    t_env.execute_sql("""
    CREATE TEMPORARY VIEW klines_view AS
    SELECT
        window_start,
        window_end,
        symbol,
        CAST(window_start AS DATE) AS landing_date,
        ROUND(FIRST_VALUE(price), 4) AS open_price,
        ROUND(MAX(price), 4) AS high_price,
        ROUND(MIN(price), 4) AS low_price,
        ROUND(LAST_VALUE(price), 4) AS close_price,
        ROUND(SUM(quantity), 1) AS volume
    FROM TABLE(
        TUMBLE(TABLE aggtrades_source, DESCRIPTOR(ts), INTERVAL '15' MINUTES)
    )
    GROUP BY window_start, window_end, symbol
    """)

    #################################################
    # 5. Define sink table using kinesis connector
    #################################################

    # --- 4Ô∏è‚É£ ClickHouse sinks ---
    t_env.execute_sql("DROP TABLE IF EXISTS klines")
    t_env.execute_sql(f"""
    CREATE TABLE klines (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE
    ) WITH (
        'connector' = 'clickhouse',
        'url' = 'clickhouse://{output_clickhouse_ip}:8123',
        'database-name' = 'testdb',
        'table-name' = 'klines',
        'username' = 'default',
        'password' = '123456',
        'sink.batch-size' = '5000',
        'sink.flush-interval' = '2s',
        'sink.max-retries' = '3',
        'sink.ignore-delete' = 'true'
    )
    """)

    t_env.execute_sql("DROP TABLE IF EXISTS indicators_clickhouse")
    t_env.execute_sql(f"""
    CREATE TABLE indicators_clickhouse (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE,
        rsi6 DOUBLE,
        rsi_ag DOUBLE,
        rsi_al DOUBLE,
        ema7 DOUBLE,
        ema20 DOUBLE,
        ema12 DOUBLE,
        ema26 DOUBLE,
        macd DOUBLE,
        signal DOUBLE,
        histogram DOUBLE,
        trend STRING,
        pattern STRING
    ) WITH (
        'connector' = 'clickhouse',
        'url' = 'clickhouse://{output_clickhouse_ip}:8123',
        'database-name' = 'testdb',
        'table-name' = 'indicators',
        'username' = 'default',
        'password' = '123456',
        'sink.batch-size' = '5000',
        'sink.flush-interval' = '2s',
        'sink.max-retries' = '3',
        'sink.ignore-delete' = 'true'
    )
    """)

    # --- 5Ô∏è‚É£ Kafka sink for real-time indicator ---
    t_env.execute_sql("DROP TABLE IF EXISTS indicators_kafka")
    t_env.execute_sql(f"""
    CREATE TABLE indicators_kafka (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE,
        rsi6 DOUBLE,
        rsi_ag DOUBLE,
        rsi_al DOUBLE,
        ema7 DOUBLE,
        ema20 DOUBLE,
        ema12 DOUBLE,
        ema26 DOUBLE,
        macd DOUBLE,
        signal DOUBLE,
        histogram DOUBLE,
        trend STRING,
        pattern STRING
    ) WITH (          
        'connector' = 'kinesis',
        'stream.arn' = '{output_stream_arn}',
        'aws.region' = '{output_stream_region}',
        'sink.partitioner-field-delimiter' = ';',
        'sink.batch.max-size' = '100',
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
    """)

    # For local development purposes, you might want to print the output to the console, instead of sending it to a
    # Kinesis Stream. To do that, you can replace the sink table using the 'kinesis' connector, above, with a sink table
    # using the 'print' connector. Comment the statement immediately above and uncomment the one immediately below.

    # table_env.execute_sql("""
    #     CREATE TABLE output (
    #             ticker VARCHAR(6),
    #             price DOUBLE,
    #             event_time TIMESTAMP(3)
    #           )
    #           WITH (
    #             'connector' = 'print'
    #           )""")

    # In a real application we would probably have some transformations between the input and the output.
    # For simplicity, we will send the source table directly to the sink table.

    ##########################################################################################
    # 6. Define an INSERT INTO statement writing data from the source table to the sink table
    ##########################################################################################

    # Executing an INSERT INTO statement will trigger the job
    # --- 8Ô∏è‚É£ Output type info ---
    typeinfo = Types.ROW_NAMED(
        [
            "window_start",
            "window_end",
            "symbol",
            "landing_date",
            "open_price",
            "high_price",
            "low_price",
            "close_price",
            "volume",
            "rsi6",
            "rsi_ag",
            "rsi_al",
            "ema7",
            "ema20",
            "ema12",
            "ema26",
            "macd",
            "signal",
            "histogram",
            "trend",
            "pattern",
        ],
        [
            Types.SQL_TIMESTAMP(),
            Types.SQL_TIMESTAMP(),
            Types.STRING(),
            Types.SQL_DATE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.STRING(),
            Types.STRING(),
        ],
    )

    # --- 9Ô∏è‚É£ Apply process ---
    klines_stream = t_env.to_data_stream(t_env.from_path("klines_view"))
    indicators_stream = klines_stream.key_by(lambda x: x["symbol"]).process(
        IndicatorsFunction(), output_type=typeinfo
    )
    t_env.drop_temporary_view("indicator_view")
    t_env.create_temporary_view(
        "indicator_view", t_env.from_data_stream(indicators_stream)
    )

    # --- üîü Execute sinks ---
    statement_set = t_env.create_statement_set()
    statement_set.add_insert_sql("INSERT INTO klines SELECT * FROM klines_view")
    statement_set.add_insert_sql(
        "INSERT INTO indicators_clickhouse SELECT * FROM indicator_view"
    )
    statement_set.add_insert_sql(
        "INSERT INTO indicators_kafka SELECT * FROM indicator_view where pattern IS NOT NULL"
    )

    # When running locally, as a standalone Python application, you must instruct Python not to exit at the end of the
    # main() method, otherwise the job will stop immediately.
    # When running the job deployed in a Flink cluster or in Amazon Managed Service for Apache Flink, the main() method
    # must end once the flow has been defined and handed over to the Flink framework to run.
    if is_local:
        statement_set.execute().wait()  # block only locally
    else:
        statement_set.execute()  # non-blocking in KDA


if __name__ == "__main__":
    logger.info("‚úÖ Starting crypto stream with logging")
    print("‚úÖ Starting crypto stream with print")
    main()
