import copy
import json
import logging
import os
import pickle

import pyflink
from pyflink.common import Row, Types
from pyflink.datastream import (
    KeyedProcessFunction,
    RuntimeContext,
    StreamExecutionEnvironment,
)
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.table import EnvironmentSettings, StreamTableEnvironment

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)  # match Kafka partitions
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Location of the configuration file when running on Managed Flink.
# NOTE: this is not the file included in the project, but a file generated by Managed Flink, based on the
# application configuration.
APPLICATION_PROPERTIES_FILE_PATH = "/etc/flink/application_properties.json"

# Set the environment variable FLINK_LOCAL=true in your local development environment,
# or in the run profile of your IDE: the application relies on this variable to run in local mode (as a standalone
# Python application, as opposed to running in a Flink cluster).
# Differently from Java Flink, PyFlink cannot automatically detect when running in local mode
is_local = True if os.environ.get("FLINK_LOCAL") else False
logger.info(f"Is local mode: {is_local} from logging")
print(f"Is local mode: {is_local} from print")

##############################################
# 2. Set special configuration for local mode
##############################################

if is_local:
    # Load the configuration from the json file included in the project
    APPLICATION_PROPERTIES_FILE_PATH = "application_properties.json"

    # Point to the fat-jar generated by Maven, containing all jar dependencies (e.g. connectors)
    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__))
    t_env.get_config().get_configuration().set_string(
        "pipeline.jars",
        # For local development (only): use the fat-jar containing all dependencies, generated by `mvn package`
        # located in the target/ subdirectory
        "file:///" + CURRENT_DIR + "/target/pyflink-dependencies.jar",
    )

    # Show the PyFlink home directory and the directory where logs will be written, when running locally
    print("PyFlink home: " + os.path.dirname(os.path.abspath(pyflink.__file__)))
    print(
        "Logging directory: "
        + os.path.dirname(os.path.abspath(pyflink.__file__))
        + "/log"
    )


# Utility method, extracting properties from the runtime configuration file
def get_application_properties():
    if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH):
        with open(APPLICATION_PROPERTIES_FILE_PATH, "r") as file:
            contents = file.read()
            properties = json.loads(contents)
            return properties
    else:
        print('A file at "{}" was not found'.format(APPLICATION_PROPERTIES_FILE_PATH))


# Utility method, extracting a property from a property group
def property_map(props, property_group_id):
    for prop in props:
        if prop["PropertyGroupId"] == property_group_id:
            return prop["PropertyMap"]


AT_TIMESTAMP = "AT_TIMESTAMP"


# --- 6Ô∏è‚É£ Utility ---
def round_half_up(x, decimals=2):
    if x is None:
        return None
    factor = 10**decimals
    return float(int(x * factor + 0.5)) / factor


# --- 7Ô∏è‚É£ Engulfing pattern function ---
class EngulfingPatternFunction(KeyedProcessFunction):
    def open(self, runtime_context: RuntimeContext):
        self.prev_row_state = runtime_context.get_state(
            ValueStateDescriptor("prev_row_state", Types.PICKLED_BYTE_ARRAY())
        )

    def calc_ema(self, close_price, period, ema_state, buffer_state):
        if close_price is None:
            return None
        k = 2 / (period + 1)
        if ema_state is None:
            buffer_state.append(close_price)
            if len(buffer_state) < period:
                return None
            ema = sum(buffer_state) / len(buffer_state)
            buffer_state.clear()
            return ema
        else:
            return (close_price - ema_state) * k + ema_state

    def detect_trend(self, ema7, ema20):
        if ema7 is None or ema20 is None:
            return None
        if ema7 > ema20:
            return "uptrend"
        elif ema7 < ema20:
            return "downtrend"
        return None

    def detect_engulfing(self, current, previous, trend):
        if not previous or not trend:
            return None
        op_prev, cp_prev = previous["open_price"], previous["close_price"]
        op, cp = current["open_price"], current["close_price"]

        if (
            cp_prev < op_prev
            and cp > op
            and op < cp_prev
            and cp > op_prev
            and trend == "downtrend"
        ):
            return "bullish engulfing"
        if (
            cp_prev > op_prev
            and cp < op
            and op > cp_prev
            and cp < op_prev
            and trend == "uptrend"
        ):
            return "bearish engulfing"
        return None

    def process_element(self, value, ctx):
        prev_bytes = self.prev_row_state.value()
        prev_state = pickle.loads(prev_bytes) if prev_bytes else {}
        buf7 = copy.deepcopy(prev_state.get("buffer7_state", []))
        buf20 = copy.deepcopy(prev_state.get("buffer20_state", []))

        ema7 = self.calc_ema(value["close_price"], 7, prev_state.get("ema7"), buf7)
        ema20 = self.calc_ema(value["close_price"], 20, prev_state.get("ema20"), buf20)
        trend = self.detect_trend(ema7, ema20)
        pattern = self.detect_engulfing(value.as_dict(), prev_state, trend)

        new_state = {
            **value.as_dict(),
            "ema7": ema7,
            "ema20": ema20,
            "trend": trend,
            "pattern": pattern,
            "buffer7_state": buf7,
            "buffer20_state": buf20,
        }
        self.prev_row_state.update(pickle.dumps(new_state))

        yield Row(
            **value.as_dict(),
            ema7=round_half_up(ema7, 4) if ema7 else None,
            ema20=round_half_up(ema20, 4) if ema20 else None,
            trend=trend,
            engulfing_pattern=pattern,
        )


def main():
    #####################################
    # 3. Retrieve runtime configuration
    #####################################

    props = get_application_properties()

    # Input stream configuration
    input_stream_properties = property_map(props, "InputStream0")
    input_stream_arn = input_stream_properties["stream.arn"]
    input_stream_region = input_stream_properties["aws.region"]
    input_stream_init_position = (
        input_stream_properties["flink.source.init.position"]
        if "flink.source.init.position" in input_stream_properties
        else None
    )
    input_stream_init_timestamp = (
        input_stream_properties["flink.source.init.timestamp"]
        if "flink.source.init.timestamp" in input_stream_properties
        else None
    )
    if (
        input_stream_init_position == AT_TIMESTAMP
        and input_stream_init_timestamp == None
    ):
        raise ValueError(
            f"A timestamp must be supplied for flink.source.init.position = {AT_TIMESTAMP}"
        )

    # Output stream configuration
    output_stream_properties = property_map(props, "OutputStream0")
    output_stream_arn = output_stream_properties["stream.arn"]
    output_stream_region = output_stream_properties["aws.region"]
    output_clickhouse_ip = output_stream_properties["clickhouse.ip"]
    logger.info(f"Output ClickHouse IP: {output_clickhouse_ip} from logging")
    print(f"Output ClickHouse IP: {output_clickhouse_ip} from print")

    #################################################
    # 4. Define source table using kinesis connector
    #################################################

    # Some trick is required to generate the string defining the initial position, depending on the configuration
    # See Flink documentation for further details about configuring a Kinesis source table
    # https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/table/kinesis/
    source_init_pos = (
        "\n'source.init.position' = '{0}',".format(input_stream_init_position)
        if input_stream_init_position is not None
        else ""
    )
    source_init_timestamp = (
        "\n'source.init.timestamp' = '{0}',".format(input_stream_init_timestamp)
        if input_stream_init_timestamp is not None
        else ""
    )

    # --- 2Ô∏è‚É£ Kafka source ---
    t_env.execute_sql("DROP TABLE IF EXISTS aggtrades_source")
    t_env.execute_sql(f"""
    CREATE TABLE aggtrades_source (
        agg_trade_id BIGINT,
        price DOUBLE,
        quantity DOUBLE,
        first_trade_id BIGINT,
        last_trade_id BIGINT,
        ts_int BIGINT,
        is_buyer_maker BOOLEAN,
        is_best_match BOOLEAN,
        symbol STRING,
        ts AS TO_TIMESTAMP_LTZ(ts_int / 1000, 3),
        WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
    ) WITH (
        'connector' = 'kinesis',
        'stream.arn' = '{input_stream_arn}',
        'aws.region' = '{input_stream_region}',
        {source_init_pos}{source_init_timestamp}
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
    """)

    # --- 3Ô∏è‚É£ Klines view ---
    t_env.execute_sql("DROP TEMPORARY VIEW IF EXISTS klines_view")
    t_env.execute_sql("""
    CREATE TEMPORARY VIEW klines_view AS
    SELECT
        window_start,
        window_end,
        symbol,
        CAST(window_start AS DATE) AS landing_date,
        ROUND(FIRST_VALUE(price), 4) AS open_price,
        ROUND(MAX(price), 4) AS high_price,
        ROUND(MIN(price), 4) AS low_price,
        ROUND(LAST_VALUE(price), 4) AS close_price,
        ROUND(SUM(quantity), 1) AS volume
    FROM TABLE(
        TUMBLE(TABLE aggtrades_source, DESCRIPTOR(ts), INTERVAL '15' MINUTES)
    )
    GROUP BY window_start, window_end, symbol
    """)

    #################################################
    # 5. Define sink table using kinesis connector
    #################################################

    # --- 4Ô∏è‚É£ ClickHouse sinks ---
    t_env.execute_sql("DROP TABLE IF EXISTS klines")
    t_env.execute_sql(f"""
    CREATE TABLE klines (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE
    ) WITH (
        'connector' = 'clickhouse',
        'url' = 'clickhouse://{output_clickhouse_ip}:8123',
        'database-name' = 'crypto_db',
        'table-name' = 'klines',
        'username' = 'admin',
        'password' = 'admin123',
        'sink.batch-size' = '5000',
        'sink.flush-interval' = '2s',
        'sink.max-retries' = '3',
        'sink.ignore-delete' = 'true'
    )
    """)

    t_env.execute_sql("DROP TABLE IF EXISTS engulfing_clickhouse")
    t_env.execute_sql(f"""
    CREATE TABLE engulfing_clickhouse (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE,
        ema7 DOUBLE,
        ema20 DOUBLE,
        trend STRING,
        engulfing_pattern STRING
    ) WITH (
        'connector' = 'clickhouse',
        'url' = 'clickhouse://{output_clickhouse_ip}:8123',
        'database-name' = 'crypto_db',
        'table-name' = 'engulfings',
        'username' = 'admin',
        'password' = 'admin123',
        'sink.batch-size' = '5000',
        'sink.flush-interval' = '2s',
        'sink.max-retries' = '3',
        'sink.ignore-delete' = 'true'
    )
    """)

    # --- 5Ô∏è‚É£ Kafka sink for real-time engulfing ---
    t_env.execute_sql("DROP TABLE IF EXISTS engulfing_kafka")
    t_env.execute_sql(f"""
    CREATE TABLE engulfing_kafka (
        window_start TIMESTAMP_LTZ(3),
        window_end TIMESTAMP_LTZ(3),
        symbol STRING,
        landing_date DATE,
        open_price DOUBLE,
        high_price DOUBLE,
        low_price DOUBLE,
        close_price DOUBLE,
        volume DOUBLE,
        ema7 DOUBLE,
        ema20 DOUBLE,
        trend STRING,
        engulfing_pattern STRING
    ) WITH (          
        'connector' = 'kinesis',
        'stream.arn' = '{output_stream_arn}',
        'aws.region' = '{output_stream_region}',
        'sink.partitioner-field-delimiter' = ';',
        'sink.batch.max-size' = '100',
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
    """)

    # For local development purposes, you might want to print the output to the console, instead of sending it to a
    # Kinesis Stream. To do that, you can replace the sink table using the 'kinesis' connector, above, with a sink table
    # using the 'print' connector. Comment the statement immediately above and uncomment the one immediately below.

    # table_env.execute_sql("""
    #     CREATE TABLE output (
    #             ticker VARCHAR(6),
    #             price DOUBLE,
    #             event_time TIMESTAMP(3)
    #           )
    #           WITH (
    #             'connector' = 'print'
    #           )""")

    # In a real application we would probably have some transformations between the input and the output.
    # For simplicity, we will send the source table directly to the sink table.

    ##########################################################################################
    # 6. Define an INSERT INTO statement writing data from the source table to the sink table
    ##########################################################################################

    # Executing an INSERT INTO statement will trigger the job
    # --- 8Ô∏è‚É£ Output type info ---
    typeinfo = Types.ROW_NAMED(
        [
            "window_start",
            "window_end",
            "symbol",
            "landing_date",
            "open_price",
            "high_price",
            "low_price",
            "close_price",
            "volume",
            "ema7",
            "ema20",
            "trend",
            "engulfing_pattern",
        ],
        [
            Types.SQL_TIMESTAMP(),
            Types.SQL_TIMESTAMP(),
            Types.STRING(),
            Types.SQL_DATE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.DOUBLE(),
            Types.STRING(),
            Types.STRING(),
        ],
    )

    # --- 9Ô∏è‚É£ Apply process ---
    klines_stream = t_env.to_data_stream(t_env.from_path("klines_view"))
    engulfing_stream = klines_stream.key_by(lambda x: x["symbol"]).process(
        EngulfingPatternFunction(), output_type=typeinfo
    )
    t_env.drop_temporary_view("engulfing_view")
    t_env.create_temporary_view(
        "engulfing_view", t_env.from_data_stream(engulfing_stream)
    )

    # --- üîü Execute sinks ---
    statement_set = t_env.create_statement_set()
    statement_set.add_insert_sql("INSERT INTO klines SELECT * FROM klines_view")
    statement_set.add_insert_sql(
        "INSERT INTO engulfing_clickhouse SELECT * FROM engulfing_view"
    )
    statement_set.add_insert_sql(
        "INSERT INTO engulfing_kafka SELECT * FROM engulfing_view where engulfing_pattern IS NOT NULL"
    )

    # When running locally, as a standalone Python application, you must instruct Python not to exit at the end of the
    # main() method, otherwise the job will stop immediately.
    # When running the job deployed in a Flink cluster or in Amazon Managed Service for Apache Flink, the main() method
    # must end once the flow has been defined and handed over to the Flink framework to run.
    if is_local:
        statement_set.execute().wait()  # block only locally
    else:
        statement_set.execute()  # non-blocking in KDA


if __name__ == "__main__":
    logger.info("‚úÖ Starting crypto stream with logging")
    print("‚úÖ Starting crypto stream with print")
    main()
