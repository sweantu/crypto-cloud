{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df3fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession, types\n",
    "from common.table import table_exists\n",
    "from common.ema import make_ema_in_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8131ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8880ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transforming symbol=ADAUSDT for date=2025-09-28\n"
     ]
    }
   ],
   "source": [
    "landing_date = \"2025-09-28\"\n",
    "symbol = \"ADAUSDT\"\n",
    "logger.info(f\"Transforming symbol={symbol} for date={landing_date}\")\n",
    "\n",
    "PROJECT_PREFIX_UNDERSCORE = os.getenv(\"PROJECT_PREFIX_UNDERSCORE\")\n",
    "DATA_LAKE_BUCKET = os.getenv(\"DATA_LAKE_BUCKET\")\n",
    "ICEBERG_LOCK_TABLE = os.getenv(\"ICEBERG_LOCK_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642239e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 16:04:36 WARN Utils: Your hostname, Nguyens-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.29 instead (on interface en0)\n",
      "25/12/05 16:04:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/anhtu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/anhtu/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-04953f11-0d9c-4f9d-bd05-dfe88fecc784;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.7.1 in central\n",
      ":: resolution report :: resolve 57ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.7.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-04953f11-0d9c-4f9d-bd05-dfe88fecc784\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/anhtu/Code/example/crypto-cloud/venv/spark_jobs/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 16:04:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Transform Job - Pattern Two\") # type: ignore\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.glue_catalog.catalog-impl\",\n",
    "        \"org.apache.iceberg.aws.glue.GlueCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.glue_catalog.lock.table\", f\"{ICEBERG_LOCK_TABLE}\")\n",
    "    # Disable vectorized reader\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "    .config(\"spark.sql.columnVector.offheap.enabled\", \"false\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"false\")\n",
    "    .config(\"spark.sql.catalog.glue_catalog.read.parquet.vectorization.enabled\", \"false\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:MaxDirectMemorySize=1g\")\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\",\n",
    "            \"org.apache.iceberg:iceberg-aws-bundle:1.7.1\",\n",
    "        ])\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input rows: 96                                                    \n"
     ]
    }
   ],
   "source": [
    "transform_db = f\"glue_catalog.{PROJECT_PREFIX_UNDERSCORE}_transform_db\"\n",
    "klines_table = \"klines\"\n",
    "sql_stmt = f\"\"\"\n",
    "select * from {transform_db}.{klines_table}\n",
    "where landing_date = DATE('{landing_date}') AND symbol = '{symbol}'\n",
    "\"\"\"\n",
    "df_sorted = (\n",
    "    spark.sql(sql_stmt)\n",
    "    .coalesce(1)  # one partition, not shuffle\n",
    "    .sortWithinPartitions(\"group_id\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Input rows: {df_sorted.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e672c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        *df_sorted.schema.fields,  # keep all original fields\n",
    "        types.StructField(\"ema7\", types.DoubleType(), True),\n",
    "        types.StructField(\"ema20\", types.DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c24f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:common.ema:Using previous EMA values: ema7=0.7825, ema20=0.7825\n"
     ]
    }
   ],
   "source": [
    "pattern_two_table = \"pattern_two\"\n",
    "if table_exists(spark, transform_db, pattern_two_table):\n",
    "    sql_stmt = f\"\"\"\n",
    "    select ema7, ema20 from {transform_db}.{pattern_two_table}\n",
    "    where landing_date = date_sub(DATE('{landing_date}'), 1) AND symbol = '{symbol}'\n",
    "    order by group_id desc\n",
    "    limit 1\n",
    "    \"\"\"\n",
    "    row = spark.sql(sql_stmt).first()\n",
    "    prev_ema7, prev_ema20 = (row[\"ema7\"], row[\"ema20\"]) if row else (None, None)\n",
    "else:\n",
    "    prev_ema7, prev_ema20 = None, None\n",
    "\n",
    "ema_in_chunks_with_state = make_ema_in_chunks(prev_ema7, prev_ema20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b075dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sorted.mapInPandas(ema_in_chunks_with_state, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f36059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Output rows: 96                                                   \n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select\n",
    "        *,\n",
    "        case \n",
    "            when ema7 > ema20 then 'uptrend' \n",
    "            when ema7 < ema20 then 'downtrend' \n",
    "            else NULL \n",
    "        end as trend,\n",
    "        LAG(open_price, 1) over(order by group_id) as open_price_prev,\n",
    "        LAG(close_price, 1) over(order by group_id) as close_price_prev\n",
    "    from temp\n",
    ")\n",
    "select\n",
    "    group_id,\n",
    "    group_date,\n",
    "    open_time,\n",
    "    open_price,\n",
    "    high_price,\n",
    "    low_price,\n",
    "    close_price,\n",
    "    volume,\n",
    "    close_time,\n",
    "    landing_date,\n",
    "    symbol,\n",
    "    ema7,\n",
    "    ema20,\n",
    "    trend,\n",
    "    case \n",
    "        when close_price_prev < open_price_prev\n",
    "            and close_price > open_price\n",
    "            and open_price < close_price_prev\n",
    "            and close_price > open_price_prev\n",
    "            and trend = 'downtrend'\n",
    "        then 'bullish engulfing'\n",
    "        when close_price_prev > open_price_prev\n",
    "            and close_price < open_price\n",
    "            and open_price > close_price_prev\n",
    "            and close_price < open_price_prev\n",
    "            and trend = 'uptrend'\n",
    "        then 'bearish engulfing'\n",
    "        else NULL\n",
    "    end as pattern\n",
    "from cte\n",
    "\"\"\")\n",
    "\n",
    "logger.info(f\"Output rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7686b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/05 16:04:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/05 16:04:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "INFO:__main__:✅Transform job completed successfully.                            \n"
     ]
    }
   ],
   "source": [
    "if table_exists(spark, transform_db, pattern_two_table):\n",
    "    df.writeTo(f\"{transform_db}.{pattern_two_table}\").overwritePartitions()\n",
    "else:\n",
    "    df.writeTo(f\"{transform_db}.{pattern_two_table}\").tableProperty(\n",
    "        \"format-version\", \"2\"\n",
    "    ).partitionedBy(\"symbol\", \"landing_date\").createOrReplace()\n",
    "\n",
    "logger.info(\"✅Transform job completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d01271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "|group_id|group_date         |open_time       |open_price|high_price|low_price|close_price|volume  |close_time      |landing_date|symbol |ema7  |ema20 |trend    |pattern          |\n",
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "|1954493 |2025-09-28 07:15:00|1759043701149668|0.7712    |0.7737    |0.7711   |0.773      |509464.1|1759044593275198|2025-09-28  |ADAUSDT|0.7725|0.7739|downtrend|bullish engulfing|\n",
      "|1954530 |2025-09-28 16:30:00|1759077000870532|0.7876    |0.788     |0.7838   |0.7857     |423035.5|1759077890920769|2025-09-28  |ADAUSDT|0.7833|0.7781|uptrend  |bearish engulfing|\n",
      "|1954413 |2025-09-27 11:15:00|1758971700405060|0.7837    |0.7853    |0.7837   |0.785      |121594.4|1758972591323187|2025-09-27  |ADAUSDT|0.7835|0.784 |downtrend|bullish engulfing|\n",
      "|1954445 |2025-09-27 19:15:00|1759000502967967|0.7803    |0.7823    |0.78     |0.7821     |95233.0 |1759001384883232|2025-09-27  |ADAUSDT|0.7808|0.7817|downtrend|bullish engulfing|\n",
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select * from {transform_db}.{pattern_two_table} where pattern is not null\n",
    "\"\"\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365487b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     192|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51316)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select count(*) from {transform_db}.{pattern_two_table}\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3e133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_jobs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
