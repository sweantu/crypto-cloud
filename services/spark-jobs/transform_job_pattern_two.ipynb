{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df3fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.errors.exceptions.base import AnalysisException\n",
    "from pyspark.sql import SparkSession, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_date = \"2025-09-27\"\n",
    "symbol = \"ADAUSDT\"\n",
    "\n",
    "PROJECT_PREFIX = os.getenv(\"PROJECT_PREFIX\")\n",
    "PROJECT_PREFIX_UNDERSCORE = os.getenv(\"PROJECT_PREFIX_UNDERSCORE\")\n",
    "DATA_LAKE_BUCKET = os.getenv(\"DATA_LAKE_BUCKET\")\n",
    "ICEBERG_LOCK_TABLE = os.getenv(\"ICEBERG_LOCK_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8131ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642239e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/anhtu/.pyenv/versions/3.11.11/envs/crypto-cloud/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/anhtu/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/anhtu/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ae39abcb-298c-4d67-ae1e-f9f1f76e991a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.7.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.7.1/iceberg-spark-runtime-3.5_2.12-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1!iceberg-spark-runtime-3.5_2.12.jar (7980ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.7.1/iceberg-aws-bundle-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-aws-bundle;1.7.1!iceberg-aws-bundle.jar (6558ms)\n",
      ":: resolution report :: resolve 714ms :: artifacts dl 14542ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.7.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ae39abcb-298c-4d67-ae1e-f9f1f76e991a\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (90206kB/91ms)\n",
      "25/11/19 23:07:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Transform Job - Pattern Two\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.glue_catalog.catalog-impl\",\n",
    "        \"org.apache.iceberg.aws.glue.GlueCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.glue_catalog.warehouse\", f\"s3://{DATA_LAKE_BUCKET}/\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\"\n",
    "    )\n",
    "    .config(\"spark.sql.catalog.glue_catalog.lock.table\", f\"{ICEBERG_LOCK_TABLE}\")\n",
    "    .config(\"spark.sql.defaultCatalog\", \"glue_catalog\")\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "    .config(\"spark.sql.columnVector.offheap.enabled\", \"false\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"false\")\n",
    "    .config(\"spark.sql.catalog.glue_catalog.read.parquet.vectorization.enabled\", \"false\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:MaxDirectMemorySize=1g\")\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\",\n",
    "            \"org.apache.iceberg:iceberg-aws-bundle:1.7.1\",\n",
    "        ])\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4c2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(spark: SparkSession, database: str, table: str) -> bool:\n",
    "    try:\n",
    "        spark.catalog.getTable(f\"{database}.{table}\")\n",
    "        return True\n",
    "    except (AnalysisException, Py4JJavaError):\n",
    "        return False\n",
    "\n",
    "\n",
    "def round_half_up(x, decimals=2):\n",
    "    if x is None:\n",
    "        return None\n",
    "    factor = 10**decimals\n",
    "    return float(int(x * factor + 0.5)) / factor\n",
    "\n",
    "\n",
    "def calc_ema(value, state):\n",
    "    if value is None:\n",
    "        return None\n",
    "    prev, buffer, period, k = (\n",
    "        state[\"prev\"],\n",
    "        state[\"buffer\"],\n",
    "        state[\"period\"],\n",
    "        state[\"k\"],\n",
    "    )\n",
    "    if prev is None:\n",
    "        buffer.append(value)\n",
    "        if len(buffer) == period:\n",
    "            ema = sum(buffer) / len(buffer)\n",
    "        else:\n",
    "            ema = None\n",
    "    else:\n",
    "        ema = (value - prev) * k + prev\n",
    "\n",
    "    state[\"prev\"] = ema\n",
    "    return ema\n",
    "\n",
    "\n",
    "def make_ema_in_chunks(prev_ema7, prev_ema20):\n",
    "    def ema_in_chunks(iterator):\n",
    "        ema_configs = {\n",
    "            \"ema7\": {\n",
    "                \"period\": 7,\n",
    "                \"k\": 2 / (7 + 1),\n",
    "                \"prev\": prev_ema7,\n",
    "                \"buffer\": [],\n",
    "            },\n",
    "            \"ema20\": {\n",
    "                \"period\": 20,\n",
    "                \"k\": 2 / (20 + 1),\n",
    "                \"prev\": prev_ema20,\n",
    "                \"buffer\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for pdf in iterator:\n",
    "            ema7, ema20 = [], []\n",
    "            for p in pdf[\"close_price\"]:\n",
    "                price = float(p)\n",
    "                e7 = calc_ema(price, ema_configs[\"ema7\"])\n",
    "                ema7.append(round_half_up(e7, 4) if e7 is not None else None)\n",
    "                e20 = calc_ema(price, ema_configs[\"ema20\"])\n",
    "                ema20.append(round_half_up(e20, 4) if e20 is not None else None)\n",
    "\n",
    "            pdf[\"ema7\"] = ema7\n",
    "            pdf[\"ema20\"] = ema20\n",
    "            pdf = pdf[[*pdf.columns[:-2], \"ema7\", \"ema20\"]]\n",
    "            yield pdf\n",
    "\n",
    "    logger.info(f\"Using previous EMA values: ema7={prev_ema7}, ema20={prev_ema20}\")\n",
    "    return ema_in_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664142ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Transforming symbol=ADAUSDT for date=2025-09-27\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Transforming symbol={symbol} for date={landing_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74c5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Input rows: 96                                                    \n"
     ]
    }
   ],
   "source": [
    "serving_db = f\"{PROJECT_PREFIX_UNDERSCORE}_serving_db\"\n",
    "klines_table = \"klines\"\n",
    "sql_stmt = f\"\"\"\n",
    "select * from {serving_db}.{klines_table}\n",
    "where landing_date = DATE('{landing_date}') AND symbol = '{symbol}'\n",
    "\"\"\"\n",
    "df_sorted = (\n",
    "    spark.sql(sql_stmt)\n",
    "    .coalesce(1)  # one partition, not shuffle\n",
    "    .sortWithinPartitions(\"group_id\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Input rows: {df_sorted.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e672c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        *df_sorted.schema.fields,  # keep all original fields\n",
    "        types.StructField(\"ema7\", types.DoubleType(), True),\n",
    "        types.StructField(\"ema20\", types.DoubleType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2c24f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using previous EMA values: ema7=None, ema20=None\n"
     ]
    }
   ],
   "source": [
    "pattern_two_table = \"pattern_two\"\n",
    "if table_exists(spark, serving_db, pattern_two_table):\n",
    "    sql_stmt = f\"\"\"\n",
    "    select ema7, ema20 from {serving_db}.{pattern_two_table}\n",
    "    where landing_date = date_sub(DATE('{landing_date}'), 1) AND symbol = '{symbol}'\n",
    "    order by group_id desc\n",
    "    limit 1\n",
    "    \"\"\"\n",
    "    row = spark.sql(sql_stmt).first()\n",
    "    prev_ema7, prev_ema20 = (row[\"ema7\"], row[\"ema20\"]) if row else (None, None)\n",
    "else:\n",
    "    prev_ema7, prev_ema20 = None, None\n",
    "\n",
    "ema_in_chunks_with_state = make_ema_in_chunks(prev_ema7, prev_ema20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b075dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sorted.mapInPandas(ema_in_chunks_with_state, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f36059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Output rows: 96                                                   \n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "with cte as (\n",
    "    select\n",
    "        *,\n",
    "        case \n",
    "            when ema7 > ema20 then 'uptrend' \n",
    "            when ema7 < ema20 then 'downtrend' \n",
    "            else NULL \n",
    "        end as trend,\n",
    "        LAG(open_price, 1) over(order by group_id) as open_price_prev,\n",
    "        LAG(close_price, 1) over(order by group_id) as close_price_prev\n",
    "    from temp\n",
    ")\n",
    "select\n",
    "    group_id,\n",
    "    group_date,\n",
    "    open_time,\n",
    "    open_price,\n",
    "    high_price,\n",
    "    low_price,\n",
    "    close_price,\n",
    "    volume,\n",
    "    close_time,\n",
    "    landing_date,\n",
    "    symbol,\n",
    "    ema7,\n",
    "    ema20,\n",
    "    trend,\n",
    "    case \n",
    "        when close_price_prev < open_price_prev\n",
    "            and close_price > open_price\n",
    "            and open_price < close_price_prev\n",
    "            and close_price > open_price_prev\n",
    "            and trend = 'downtrend'\n",
    "        then 'bullish engulfing'\n",
    "        when close_price_prev > open_price_prev\n",
    "            and close_price < open_price\n",
    "            and open_price > close_price_prev\n",
    "            and close_price < open_price_prev\n",
    "            and trend = 'uptrend'\n",
    "        then 'bearish engulfing'\n",
    "        else NULL\n",
    "    end as pattern\n",
    "from cte\n",
    "\"\"\")\n",
    "\n",
    "logger.info(f\"Output rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7686b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 23:08:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/11/19 23:08:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "INFO:__main__:Transform job completed successfully.                             \n"
     ]
    }
   ],
   "source": [
    "if table_exists(spark, serving_db, pattern_two_table):\n",
    "    df.writeTo(f\"{serving_db}.{pattern_two_table}\").overwritePartitions()\n",
    "else:\n",
    "    df.writeTo(f\"{serving_db}.{pattern_two_table}\").tableProperty(\n",
    "        \"format-version\", \"2\"\n",
    "    ).partitionedBy(\"symbol\", \"landing_date\").createOrReplace()\n",
    "\n",
    "logger.info(\"Transform job completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d01271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "|group_id|group_date         |open_time       |open_price|high_price|low_price|close_price|volume  |close_time      |landing_date|symbol |ema7  |ema20 |trend    |pattern          |\n",
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "|1954413 |2025-09-27 11:15:00|1758971700405060|0.7837    |0.7853    |0.7837   |0.785      |121594.4|1758972591323187|2025-09-27  |ADAUSDT|0.7835|0.784 |downtrend|bullish engulfing|\n",
      "|1954445 |2025-09-27 19:15:00|1759000502967967|0.7803    |0.7823    |0.78     |0.7821     |95233.0 |1759001384883232|2025-09-27  |ADAUSDT|0.7808|0.7817|downtrend|bullish engulfing|\n",
      "+--------+-------------------+----------------+----------+----------+---------+-----------+--------+----------------+------------+-------+------+------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serving_db = f\"{PROJECT_PREFIX_UNDERSCORE}_serving_db\"\n",
    "pattern_two_table = \"pattern_two\"\n",
    "spark.sql(f\"\"\"\n",
    "select * from {serving_db}.{pattern_two_table} where pattern is not null\n",
    "\"\"\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365487b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      96|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "select count(*) from {serving_db}.{pattern_two_table}\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3e133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto-cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
